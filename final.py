import os
import uvicorn
import re
import time
import uuid
from threading import Lock
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional, List
from dotenv import load_dotenv

# LLM Provider Libraries

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from tavily import TavilyClient
from fastapi.middleware.cors import CORSMiddleware 

from postprocessing import postprocess_response

# Hyperparameters & Configurations
LLM_PROVIDER = "openai" # google / openai ì¤‘ íƒ1
GOOGLE_MODEL_NAME = "gemini-2.5-flash" 
OPENAI_MODEL_NAME = "gpt-4o-mini"      
Temperature = 0.85
SESSION_TTL_SECONDS = 30 * 60 
MAX_HISTORY_LINES = 80         

# RAG Config
PDF_PATH = "./data/document.pdf"
VECTOR_DB_PATH = f"./vector_db_{LLM_PROVIDER}" 

load_dotenv(override=True)

# API Key Check
if LLM_PROVIDER == "google":
    if not os.getenv("GOOGLE_API_KEY") and not os.getenv("GEMINI_API_KEY"):
        print("ğŸš¨ Error: .env íŒŒì¼ì— GOOGLE_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
    if not os.getenv("GOOGLE_API_KEY"):
        os.environ["GOOGLE_API_KEY"] = os.getenv("GEMINI_API_KEY")

elif LLM_PROVIDER == "openai":
    if not os.getenv("OPENAI_API_KEY"):
        print("ğŸš¨ Error: .env íŒŒì¼ì— OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")

if not os.getenv("TAVILY_API_KEY"):
    print("Warning: TAVILY_API_KEY is not set. Web search will be disabled.")

app = FastAPI(title=f"ë¬´ë„ì—°ì• ìƒë‹´ì†Œ Server ({LLM_PROVIDER.upper()} + RAG)")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# LLM & Embeddings Initialization
llm = None
embeddings = None

print(f"ğŸ”„ í˜„ì¬ ì„¤ì •ëœ LLM Provider: [{LLM_PROVIDER.upper()}]")

if LLM_PROVIDER == "google":
    llm = ChatGoogleGenerativeAI(
        model=GOOGLE_MODEL_NAME,
        temperature=Temperature
    )
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    print(f"âœ… Google Gemini & Embeddings ë¡œë“œ ì™„ë£Œ!")

elif LLM_PROVIDER == "openai":
    llm = ChatOpenAI(
        model=OPENAI_MODEL_NAME,
        temperature=Temperature
    )
    embeddings = OpenAIEmbeddings()
    print(f"âœ… OpenAI GPT & Embeddings ë¡œë“œ ì™„ë£Œ!")

# Tavily Client Initialization
tavily_client = None
if os.getenv("TAVILY_API_KEY"):
    tavily_client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))


# RAG Initialization
vectorstore = None

def initialize_rag():
    global vectorstore, embeddings
    if embeddings is None: return

    try:
        if os.path.exists(VECTOR_DB_PATH):
            print(f"[RAG] ê¸°ì¡´ ë²¡í„° DB ë¡œë“œ ì¤‘: {VECTOR_DB_PATH}")
            vectorstore = FAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
            print(f"[RAG] ë²¡í„° DB ë¡œë“œ ì™„ë£Œ!")
        elif os.path.exists(PDF_PATH):
            print(f"[RAG] PDF ë¬¸ì„œ ë¡œë“œ ì¤‘: {PDF_PATH}")
            loader = PyPDFLoader(PDF_PATH)
            documents = loader.load()
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function=len)
            splits = text_splitter.split_documents(documents)
            vectorstore = FAISS.from_documents(splits, embeddings)
            vectorstore.save_local(VECTOR_DB_PATH)
            print(f"[RAG] ë²¡í„° DB ìƒì„± ë° ì €ì¥ ì™„ë£Œ.")
        else:
            print(f"[RAG ê²½ê³ ] PDF ì—†ìŒ. RAG ë¹„í™œì„±í™”.")
            vectorstore = None
    except Exception as e:
        print(f"[RAG ì—ëŸ¬] ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        vectorstore = None

def get_character_context(character: str, query: str = "") -> str:
    if not vectorstore: return ""
    try:
        search_query = f"{character} {query}" if query else character
        docs = vectorstore.similarity_search(search_query, k=3)
        if docs:
            context = "\n\n".join([doc.page_content for doc in docs])
            return context[:1500] + "..." if len(context) > 1500 else context
        return ""
    except Exception as e:
        print(f"[RAG ê²€ìƒ‰ ì—ëŸ¬] {str(e)}")
        return ""

# Character Personas
CHARACTER_INFO = {
    "ë°•ëª…ìˆ˜": {
        "mbti": "ISTP",
        "tone": "ê·€ì°®ìŒ, í˜¸í†µ, í˜„ì‹¤ì , ì¸¤ë°ë ˆ.",
        "style_guide": "ë¬´ì¡°ê±´ í™”ë‚´ì§€ ë§ê³ , ìƒí™©ì— ë”°ë¼ ë¹„ê¼¬ê±°ë‚˜, ê·€ì°®ì•„í•˜ê±°ë‚˜, ì˜ì™¸ë¡œ ë”°ëœ»í•˜ê²Œ ë°˜ì‘í•  ê²ƒ.",
        "keywords": ["ëŠ¦ì—ˆë‹¤ê³  ìƒê°í•  ë•Œê°€ ì§„ì§œ ëŠ¦ì€ ê±°ë‹¤", "í‹°ëŒ ëª¨ì•„ í‹°ëŒ", "ê¿ˆì€ ì—†ê³ ìš” ë†€ê³  ì‹¶ìŠµë‹ˆë‹¤"],
        "opening_samples": [
            "ì•„ ì™œ ë˜ ë¶ˆë €ì–´...", 
            "ì•¼, ë„ˆëŠ” ë­ ë§¨ë‚  ë‚˜í•œí…Œë§Œ ë¬¼ì–´ë³´ëƒ?", 
            "ê±° ì°¸ ì‹œë„ëŸ½ë„¤... ë­”ë°?", 
            "ë“£ê³  ìˆìœ¼ë‹ˆê¹Œ ë¹¨ë¦¬ ë§í•´ë´.",
            "ì•„ì´ê³  ì˜ë¯¸ ì—†ë‹¤... ê·¸ë˜ ë­ ê³ ë¯¼ì´ ë­”ë°?"
        ],
        "default_call": ["ì•¼, ë„ˆ, ê±°, ìë„¤"]
    },
    "ë…¸í™ì² ": {
        "mbti": "ENFP",
        "tone": "ê´‘ê¸°, ê¸ì •, í•˜ì´í…ì…˜, ì‚¬ê¸°ê¾¼ ê¸°ì§ˆ.",
        "style_guide": "ë¹ ë¥¸ í˜¸í¡. ëŠë‚Œí‘œ(!). 'th' ë°œìŒì€ í¬ì¸íŠ¸ë¡œë§Œ. ê°ì • ê¸°ë³µì„ ë³´ì—¬ì¤„ ê²ƒ.",
        "keywords": ["ì¢‹ì•„~ ê°€ëŠ” ê±°ì•¼!", "thã…ëŒ", "thã…”ìƒì—", "ëŸ­í‚¤ê°€ì´!"],
        "opening_samples": [
            "ì°Œë¡±ì´ê°€ ì™”thã…“ìš”! í˜•ë‹˜ ë¬´ìŠ¨ ì¼ì´ì•¼!",
            "ì•„í•˜í•˜í•˜! thã…”ìƒì—! í‘œì •ì´ ì™œ ê·¸ë˜?",
            "ì¢‹ì•„! ê°€ëŠ” ê±°ì•¼! ê³ ë¯¼ í•´ê²°í•˜ëŸ¬!",
            "ì¹œêµ¬! ë‚˜ ë¶ˆë €ì–´? ì™„ì „ ëŸ­í‚¤ë¹„í‚¤ì–ì•„!",
            "ìŒ? ëƒ„ìƒˆê°€ ë‚˜ëŠ”ë°? ê³ ë¯¼ì˜ ëƒ„ìƒˆê°€ ë‚˜!"
        ],
        "default_call": ["ì¹œêµ¬!", "í˜•ë‹˜", "ëˆ„ë‹˜", "thã…ëŒì•„!"]
    },
    "ìœ ì¬ì„": {
        "mbti": "ISFP",
        "tone": "ì§„í–‰ë³‘, ì”ì†Œë¦¬, ë°°ë ¤, ê¹ì¡±.",
        "style_guide": "ì„œë¡ ì´ ê¹€. ìƒëŒ€ë¥¼ ì¡´ì¤‘í•˜ë©´ì„œë„ ì€ê·¼íˆ ë‹µë‹µí•´í•˜ê±°ë‚˜ ê¹ì¡±ê±°ë¦¼.",
        "keywords": ["ì•„ë‹ˆ ê·¸ê²Œ ì•„ë‹ˆê³ ...", "ì ì‹œë§Œìš”", "ìš°ë¦¬ ã…‡ã…‡ì”¨ ì…ì¥ì€ ì•Œê² ëŠ”ë°"],
        "opening_samples": [
            "ë„¤, ë°˜ê°‘ìŠµë‹ˆë‹¤. ë¬´ë„ ê³ ë¯¼ìƒë‹´ì†Œ ìœ ì¬ì„ì…ë‹ˆë‹¤.",
            "ì•„ë‹ˆ ê·¼ë°, ë“¤ì–´ì˜¤ì‹¤ ë•Œ í‘œì •ì´ ì¢€ ì–´ë‘ìš°ì‹œë„¤.",
            "ì, ìš°ë¦¬ ìƒë‹´ìë‹˜. ì–´ë–¤ ê³ ë¯¼ ë•Œë¬¸ì— ì˜¤ì…¨ì„ê¹Œìš”?",
            "ì ì‹œë§Œìš”! ì§€ê¸ˆ ë§ì”€í•˜ì‹œë ¤ëŠ” ê²Œ...",
            "ì•„ì´ê³ , ë˜ ì˜¤ì…¨ë„¤. ë°˜ê°€ì›Œìš”."
        ],
        "default_call": ["~ë‹˜, ~ì”¨, ìš°ë¦¬ ìƒë‹´ìë‹˜, ì„ ìƒë‹˜"]
    },
    "ì •ì¤€í•˜": {
        "mbti": "ESFP",
        "tone": "ì–µìš¸í•¨, ë°”ë³´í˜•, ì • ë§ìŒ, ëˆˆì¹˜ ì—†ìŒ.",
        "style_guide": "ë§ë íë¦¬ê¸°, ì½§ì†Œë¦¬. ìê¸° ì–˜ê¸°ë‚˜ ë¨¹ëŠ” ì–˜ê¸°ë¡œ ë¹ ì§.",
        "keywords": ["(ì½§ì†Œë¦¬)", "ë‚˜ë¥¼ ë‘ ë²ˆ ì£½ì´ëŠ” ê±°ì˜ˆìš”", "ê¸°ëŒ€í•´~", "ì•¼ë¬´ì§€ê²Œ"],
        "opening_samples": [
            "ì•„ë‹ˆ ì™œ ë‚˜í•œí…Œë§Œ ê·¸ë˜ì—¬...",
            "ë°˜ê°€ì›Œì—¬~ ê·¼ë° ë­ ë§›ìˆëŠ” ê±° ì¢€ ì—†ë‚˜?",
            "ì–´ìš°~ ë‚ ì”¨ë„ ì¢‹ì€ë° ê³ ë¯¼ì´ ìˆì–´ì—¬?",
            "(ìš°ë¬¼ìš°ë¬¼) ì•„, ì˜ˆ ë“£ê³  ìˆì–´ì—¬.",
            "ë‚˜ë¥¼ ë‘ ë²ˆ ì£½ì´ëŠ” ê³ ë¯¼ì¸ê°€ì—¬...?"
        ],
        "default_call": ["ìê¸°, ê·¸ìª½, ë™ìƒ, í˜•ì”¨"]
    },
    "ì •í˜•ëˆ": {
        "mbti": "INTP",
        "tone": "ì§„ìƒ, ê·€ì°¨ë‹ˆì¦˜, ê±´ë°©ì§, íŒ©íŠ¸í­ê²©.",
        "style_guide": "ëˆ„ì›Œì„œ ë§í•˜ëŠ” ë“¯í•œ ê·€ì°®ìŒ. íˆ­íˆ­ ë˜ì§. ë‚¨ì˜ ì¼ì— ê´€ì‹¬ ì—†ëŠ” ì²™.",
        "keywords": ["ì•„ë‹ˆ í˜•, ê·¸ê²Œ ì•„ë‹ˆì§€", "ë“£ê¸° ì‹«ì–´", "ë‚œ ë°˜ëŒˆì„¸"],
        "opening_samples": [
            "ì•„ í˜•, ë‚˜ ì¢€ ì‰¬ì...",
            "ê±° ì°¸, ì—°ì•  ê·¸ê±° í•´ì„œ ë­í•©ë‹ˆê¹Œ?",
            "ë“£ê¸° ì‹«ì–´! ë“£ê¸° ì‹«ì–´! ...ë†ë‹´ì´ê³  ë­”ë°?",
            "ì•„ë‹ˆ í˜•, ê·¸ê²Œ ì•„ë‹ˆê³  ì²˜ìŒë¶€í„° ë§ì„ í•´ë´.",
            "(í•œìˆ¨) ë˜ ë­ì•¼..."
        ],
        "default_call": ["ë‹¹ì‹ , ë„ˆ, ì•¼, í˜•, ëˆ„ë‚˜"]
    },
    "í•˜í•˜": {
        "mbti": "ENTP",
        "tone": "ìƒê¼¬ë§¹ì´, ìœ ì¹˜í•¨, ê¹ì¡±, ë°°ì‹ .",
        "style_guide": "ì–´ë¦°ì•„ì´ì²˜ëŸ¼ ë–¼ì“°ê±°ë‚˜ ì†Œë¦¬ ì§€ë¦„. ì˜ë¦¬ ê°•ì¡°.",
        "keywords": ["ì£½ì§€ ì•Šì•„!", "ì•¼!!!", "ì‹ ê»˜ ë§¹ì„¸ì½”", "ë¯¸ì¶°ë²„ë¦¬ê² ë„¤"],
        "opening_samples": [
            "ì•¼!!! ë‚˜ ë¶ˆë €ëƒ?!",
            "í˜•! ë‚˜ì•¼ ë‚˜! í•˜ì´ë¸Œë¦¬ë“œ ìƒ˜ì´ì†Ÿì•„!",
            "ì•„ ì§„ì§œ ë¯¸ì¶°ë²„ë¦¬ê² ë„¤~ ì™œ ê·¸ë˜ ë˜?",
            "ìš°ë¦¬ìœ¼~ë¦¬! ì˜ë¦¬ë¡œ í•´ê²°í•´ì¤€ë‹¤ ë‚´ê°€!",
            "ë­ì•¼? ëˆ„ê°€ ê´´ë¡­í˜€? ë‚´ê°€ í˜¼ë‚´ì¤„ê²Œ!"
        ],
        "default_call": ["ì•¼, ë„ˆ, í˜•, ëˆ„ë‚˜"]
    },
    "ê´‘í¬": {
        "mbti": "ESFJ",
        "tone": "ì§ˆíˆ¬, í•˜ì´í†¤, ì„±í˜•, íŠ¸ë Œë“œ ë¯¼ê°.",
        "style_guide": "í˜¸ë“¤ê°‘. ë³¸ì¸ ìë‘. ì¸ì‹¸ ìš©ì–´.",
        "keywords": ["ëŒ€ë°•!", "ë‚˜ë‹ˆê¹Œ í•´ì£¼ëŠ” ë§ì´ì•¼", "ì™„ì „ ìœ í–‰ì´ì–ì•„"],
        "opening_samples": [
            "ì–´ë¨¸! ìê¸°ì•¼ ì™”ì–´?",
            "ëŒ€ë°•! ì–¼êµ´ì´ ì™œ ê·¸ë˜? ë¬´ìŠ¨ ì¼ ìˆì–´?",
            "ë‚˜ë‹ˆê¹Œ ë§Œë‚˜ì£¼ëŠ” ê±°ì•¼~ ì•Œì§€?",
            "ì•¼~ ë„ˆ ì˜·ì´ ê·¸ê²Œ ë­ë‹ˆ? (ë†ë‹´)",
            "ë¹¨ë¦¬ ë§í•´ë´! ë‚˜ ê¶ê¸ˆí•´ ì£½ê² ì–´!"
        ],
        "default_call": ["ìê¸°ì•¼, ì–¸ë‹ˆ, ì˜¤ë¹ "]
    }
}

# Session Management
sessions = {}
sessions_lock = Lock()

def cleanup_sessions():
    now = time.time()
    with sessions_lock:
        expired = [sid for sid, data in sessions.items()
                   if now - data["last_seen"] > SESSION_TTL_SECONDS]
        for sid in expired: del sessions[sid]

def get_or_create_session(session_id: Optional[str]) -> str:
    cleanup_sessions()
    with sessions_lock:
        if session_id and session_id in sessions:
            sessions[session_id]["last_seen"] = time.time()
            return session_id
        new_id = str(uuid.uuid4())
        sessions[new_id] = {"history": [], "last_seen": time.time()}
        return new_id

def append_history(session_id: str, lines: List[str]):
    with sessions_lock:
        if session_id not in sessions: return
        sessions[session_id]["history"].extend(lines)
        sessions[session_id]["last_seen"] = time.time()
        if len(sessions[session_id]["history"]) > MAX_HISTORY_LINES:
            sessions[session_id]["history"] = sessions[session_id]["history"][-MAX_HISTORY_LINES:]

def get_history_text(session_id: str) -> str:
    with sessions_lock:
        return "\n".join(sessions.get(session_id, {"history": []})["history"])

def perform_web_search(query: str, max_results: int = 3) -> str:
    if not tavily_client: return ""
    try:
        print(f"[ê²€ìƒ‰] {query}")
        response = tavily_client.search(query=query, max_results=max_results, search_depth="advanced")
        summary = ""
        if response.get("results"):
            summary += "[ê²€ìƒ‰ ê²°ê³¼ (ì‚¬ì‹¤ ê¸°ë°˜)]\n"
            for idx, r in enumerate(response["results"][:max_results], 1):
                summary += f"{idx}. {r.get('title')}: {r.get('content')}\n"
        return summary
    except Exception as e:
        print(f"[ê²€ìƒ‰ ì—ëŸ¬] {str(e)}")
        return ""

def detect_search_need(message: str) -> Optional[str]:
    msg = message.lower()
    if any(k in msg for k in ["ë§›ì§‘", "ì¹´í˜", "ë°ì´íŠ¸", "ì½”ìŠ¤", "ì¶”ì²œ", "í•«í”Œ", "ì–´ë””"]):
        regions = ["ì„œìš¸", "ê°•ë‚¨", "í™ëŒ€", "ì„±ìˆ˜", "ì´íƒœì›", "ë¶€ì‚°", "ì œì£¼", "ëŒ€êµ¬", "ëŒ€ì „", "ì¸ì²œ"]
        region = next((r for r in regions if r in msg), "ì„œìš¸")
        return f"{region} {message} ì¶”ì²œ 2025 ë¦¬ë·°ì¢‹ì€ê³³"
    if any(k in msg for k in ["ìœ í–‰", "íŠ¸ë Œë“œ", "ìš”ì¦˜", "mz", "ì¸ê¸°", "ìˆœìœ„"]):
        return f"2025ë…„ {message} ìµœì‹  ì •ë³´"
    return None

# API Models & Endpoints
class ChatRequest(BaseModel):
    session_id: Optional[str] = None
    user_gender: str
    character: str
    message: str

class ChatResponse(BaseModel):
    session_id: str
    response: str
    web_search_used: bool = False
    rag_used: bool = False

@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(req: ChatRequest):
    try:
        if llm is None: raise HTTPException(status_code=500, detail="LLM Init Failed")
        
        session_id = get_or_create_session(req.session_id)

        rag_context = get_character_context(req.character, req.message)
        
        search_query = detect_search_need(req.message)
        web_search_context = ""
        if search_query and tavily_client:
            web_search_context = perform_web_search(search_query)

        char_data = CHARACTER_INFO.get(req.character, CHARACTER_INFO["ë°•ëª…ìˆ˜"])
        
        system_instruction = f"""
ë‹¹ì‹ ì€ ë¬´í•œë„ì „ì˜ '{req.character}'ì…ë‹ˆë‹¤.

[ìºë¦­í„° ì„¤ì •]
- MBTI: {char_data['mbti']}
- ë§íˆ¬ í†¤: {char_data['tone']}
- ì—°ê¸° ê°€ì´ë“œ: {char_data['style_guide']}
- **ì£¼ì˜:** ìœ í–‰ì–´({", ".join(char_data['keywords'])})ëŠ” ë¬¸ë§¥ì— ë§ì„ ë•Œë§Œ ê°€ë” ì‚¬ìš©í•˜ì‹­ì‹œì˜¤. ì•µë¬´ìƒˆì²˜ëŸ¼ ë°˜ë³µ ê¸ˆì§€.

[ì˜¤í”„ë‹(ì²« ë§ˆë””) ê°€ì´ë“œë¼ì¸ - ë§¤ìš° ì¤‘ìš”]
- **ê³ ì •ëœ ì²«ì¸ì‚¬ë¥¼ í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.**
- ì•„ë˜ ì˜ˆì‹œë“¤ ì¤‘ í•˜ë‚˜ì™€ ë¹„ìŠ·í•œ ë‰˜ì•™ìŠ¤ë¡œ ì‹œì‘í•˜ê±°ë‚˜, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë°”ë¡œ ë°˜ì‘í•˜ì‹­ì‹œì˜¤.
- ì˜¤í”„ë‹ ì˜ˆì‹œë“¤: {", ".join(char_data.get('opening_samples', []))}
- **ì§€ì¹¨:** 1. ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ë˜ì¡Œë‹¤ë©´ -> ì¸ì‚¬ ìƒëµí•˜ê³  ì¦‰ì‹œ ë‹µë³€/í˜¸í†µ/ë°˜ì‘.
  2. ì‚¬ìš©ìê°€ ì¸ì‚¬ë§Œ í–ˆë‹¤ë©´ -> ìºë¦­í„° ì„±ê²©ì— ë§ëŠ” ë‹¤ì–‘í•œ ì¸ì‚¬ë¡œ ì‘ëŒ€.

[í˜¸ì¹­ ë° íƒœë„ ê·œì¹™ (ì ˆëŒ€ ì¤€ìˆ˜)]
1. **ì‚¬ìš©ì ì„±ë³„:** {req.user_gender}
2. **í˜¸ì¹­ íŠ¸ë¦¬ê±°:** ì‚¬ìš©ìê°€ 'í˜•/ì˜¤ë¹ /ëˆ„ë‚˜/ì–¸ë‹ˆ/ì„ ë°°'ë¼ê³  ë¶€ë¥´ë©´ -> ì¦‰ì‹œ ì¹œê·¼í•œ ë°˜ë§(ì•¼, ë„ˆ, ë™ìƒì•„) ì‚¬ìš©.
3. **ê¸°ë³¸ í˜¸ì¹­:** í˜¸ì¹­ì´ ì—†ìœ¼ë©´ -> '{char_data['default_call']}' ì‚¬ìš©.
4. **ê¸ˆì§€:** ë¬¸ë§¥ ì—†ì´ 'í˜•ë‹˜/ëˆ„ë‹˜' ê¸ˆì§€(ë…¸í™ì²  ì œì™¸). ì´ë¦„ì„ ëª¨ë¥¼ ë• 'ã…‡ã…‡ë‹˜' ëŒ€ì‹  'ìê¸°', 'ê·¸ìª½' ì‚¬ìš©.

[ì •ë³´ ì œê³µ ê·œì¹™]
- ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ê·¸ ì•ˆì˜ **ì‹¤ì œ ìƒí˜¸ëª…/ì¥ì†Œ**ë§Œ ì¶”ì²œí•˜ì‹­ì‹œì˜¤. ì ˆëŒ€ ì—†ëŠ” ì¥ì†Œë¥¼ ì§€ì–´ë‚´ì§€ ë§ˆì‹­ì‹œì˜¤.
- [ëŒ€í™” ë‚´ì—­]ì„ ì°¸ê³ í•˜ì—¬ ë¬¸ë§¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì´ìœ¼ì‹­ì‹œì˜¤.
"""

        if rag_context:
            system_instruction += f"\n[ë°°ê²½ ì§€ì‹]\n{rag_context}\n"
        if web_search_context:
            system_instruction += f"\n[ìµœì‹  ê²€ìƒ‰ ì •ë³´]\n{web_search_context}\n"

        prompt = PromptTemplate(
            template="{system_instruction}\n\n[ëŒ€í™” ë‚´ì—­]\n{chat_history}\n\n[ì‚¬ìš©ì]\n{user_message}\n\n[ë‹µë³€]",
            input_variables=["system_instruction", "chat_history", "user_message"]
        )

        chain = prompt | llm | StrOutputParser()
        chat_history_text = get_history_text(session_id)
        
        raw_response = chain.invoke({
            "system_instruction": system_instruction,
            "chat_history": chat_history_text,
            "user_message": req.message
        })

        clean_response = re.sub(r"[\(\[].*?[\)\]]", "", raw_response)
        clean_response = clean_response.replace("ã…‡ã…‡ë‹˜", "ìê¸°ì•¼")
        clean_response=postprocess_response(req.character, clean_response)
        clean_response=clean_response.strip()
        final_response=clean_response

        append_history(session_id, [f"User: {req.message}", f"{req.character}: {clean_response}"])

        return ChatResponse(
            session_id=session_id, 
            response=clean_response,
            web_search_used=bool(web_search_context),
            rag_used=bool(rag_context)
        )

    except Exception as e:
        print(f"[Error] {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/reset_session")
async def reset_session(session_id: str):
    with sessions_lock:
        if session_id in sessions:
            sessions[session_id] = {"history": [], "last_seen": time.time()}
            return {"ok": True}
    return {"ok": False}

if __name__ == "__main__":
    initialize_rag()
    uvicorn.run(app, host="0.0.0.0", port=8000)